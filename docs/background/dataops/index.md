# DataOps Overview

**DataOps** is a term describing the application of the principles of _DevOps_ to data-centric software and information technology processes. In this lesson, we will provide a brief background of DevOps, its evolution to DataOps, and how we can use these principles to guide our own development processes and architectures.

The organization _DataKitchen_ has published a [DataOps Manifesto](https://dataopsmanifesto.org/en/) in the tradition of similar "manifestos" such as the [Agile Manifesto](https://agilemanifesto.org/) and the [Zen of Python](https://www.python.org/dev/peps/pep-0020/). Here are some selected points:

>  1. **Continually satisfy your customer**: Our highest priority is to satisfy the customer through the early and continuous delivery of valuable analytic insights from a couple of minutes to weeks.
>  3. **Embrace change**: We welcome evolving customer needs, and in fact, we embrace them to generate competitive advantage. We believe that the most efficient, effective, and agile method of communication with customers is face-to-face conversation.
>  4. **It’s a team sport**: Analytic teams will always have a variety of roles, skills, favorite tools, and titles. A diversity of backgrounds and opinions increases innovation and productivity.
>  5. **Daily interactions**: Customers, analytic teams, and operations must work together daily throughout the project.
>  7. **Reduce heroism**: As the pace and breadth of need for analytic insights ever increases, we believe analytic teams should strive to reduce heroism and create sustainable and scalable data analytic teams and processes.
>  9. **Analytics is code**: Analytic teams use a variety of individual tools to access, integrate, model, and visualize data. Fundamentally, each of these tools generates code and configuration which describes the actions taken upon data to deliver insight.
> 10. **Orchestrate**: The beginning-to-end orchestration of data, tools, code, environments, and the analytic teams work is a key driver of analytic success.
> 11. **Make it reproducible**: Reproducible results are required and therefore we version everything: data, low-level hardware and software configurations, and the code and configuration specific to each tool in the toolchain.
> 12. **Disposable environments**: We believe it is important to minimize the cost for analytic team members to experiment by giving them easy to create, isolated, safe, and disposable technical environments that reflect their production environment.
> 13. **Simplicity**: We believe that continuous attention to technical excellence and good design enhances agility; likewise simplicity–the art of maximizing the amount of work not done–is essential.
> 15. **Quality is paramount**: Analytic pipelines should be built with a foundation capable of automated detection of abnormalities (jidoka) and security issues in code, configuration, and data, and should provide continuous feedback to operators for error avoidance (poka yoke).
> 18. **Improve cycle times**: We should strive to minimize the time and effort to turn a customer need into an analytic idea, create it in development, release it as a repeatable production process, and finally refactor and reuse that product.

[Reaction Whiteboard](https://gowustl-my.sharepoint.com/:wb:/g/personal/t_schlic_wustl_edu/ETpj7j6zr69Bu2rIAEmOBCsBCjYpZATXNf_XsOLDXuxAHQ?e=V6JMT6){:target="_blank"}

!!! abstract "Further Reading"

    For a more in-depth analysis of these topics and DataOps in general, see [What is DataOps?](https://datakitchen.io/what-is-dataops/) from DataKitchen.
